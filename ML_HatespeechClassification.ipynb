{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hate Speech on Twitter Assignment\n",
    "\n",
    "This is an assignment from a machine learning class where we had to analyze and recognize hate speech.\n",
    "\n",
    "#### Disclaimer:\n",
    "These tweets can be harmful. Avoid reading tweets or outputs from code if you could be offended by this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "# Standard plot size\n",
    "plt.rcParams['figure.figsize'] = (15, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we pre-processed these tweets by:\n",
    "\n",
    "- removing unwanted characters, explicit mentions, urls and hashtags (they have been replaced by dummy tags, see below)\n",
    "\n",
    "- automatically tokenizing the text and annotating the grammatical category and lemma of each token\n",
    "\n",
    "These data are available in the `labeled_data_preprocessed.csv` file available in the `data` folder, whose structure looks as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>raw_tweet</th>\n",
       "      <th>pos_tagged_tweet</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...</td>\n",
       "      <td>[(!, !), (!, !), (!, !), (rt, rt), (mentionher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...</td>\n",
       "      <td>[(!, !), (!, !), (!, !), (rt, rt), (mentionher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>[(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...</td>\n",
       "      <td>[(!, !), (!, !), (!, !), (rt, rt), (mentionher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>[(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...</td>\n",
       "      <td>[(!, !), (!, !), (!, !), (rt, rt), (mentionher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>[(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...</td>\n",
       "      <td>[(!, !), (!, !), (!, !), (rt, rt), (mentionher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...</td>\n",
       "      <td>[(!, .), (!, .), (!, .), (mentionhere, NOUN), ...</td>\n",
       "      <td>[(!, !), (!, !), (!, !), (mentionhere, mention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!\"@__BrighterDays: I can not just sit up ...</td>\n",
       "      <td>[(!, .), (!, .), (!, .), (mentionhere, NOUN), ...</td>\n",
       "      <td>[(!, !), (!, !), (!, !), (mentionhere, mention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!&amp;#8220;@selfiequeenbri: cause I'm tired of...</td>\n",
       "      <td>[(!, .), (!, .), (!, .), (mentionhere, ADV), (...</td>\n",
       "      <td>[(!, !), (!, !), (!, !), (mentionhere, mention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" &amp;amp; you might not get ya bitch back &amp;amp; ...</td>\n",
       "      <td>[(&amp;, CONJ), (you, PRON), (might, VERB), (not, ...</td>\n",
       "      <td>[(&amp;, &amp;), (you, you), (might, might), (not, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" @rhythmixx_ :hobbies include: fighting Maria...</td>\n",
       "      <td>[(mentionhere, ADV), (:, .), (hobbies, NOUN), ...</td>\n",
       "      <td>[(mentionhere, mentionhere), (:, :), (hobbies,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" Keeks is a bitch she curves everyone \" lol I...</td>\n",
       "      <td>[(keeks, NOUN), (is, VERB), (a, DET), (bitch, ...</td>\n",
       "      <td>[(keeks, keeks), (is, be), (a, a), (bitch, bit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" Murda Gang bitch its Gang Land \"</td>\n",
       "      <td>[(murda, NOUN), (gang, NOUN), (bitch, VERB), (...</td>\n",
       "      <td>[(murda, murda), (gang, gang), (bitch, bitch),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\" So hoes that smoke are losers ? \" yea ... go...</td>\n",
       "      <td>[(so, ADV), (hoes, VERB), (that, ADP), (smoke,...</td>\n",
       "      <td>[(so, so), (hoes, hoe), (that, that), (smoke, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" bad bitches is the only thing that i like \"</td>\n",
       "      <td>[(bad, ADJ), (bitches, NOUN), (is, VERB), (the...</td>\n",
       "      <td>[(bad, bad), (bitches, bitch), (is, be), (the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" bitch get up off me \"</td>\n",
       "      <td>[(bitch, NOUN), (get, VERB), (up, PRT), (off, ...</td>\n",
       "      <td>[(bitch, bitch), (get, get), (up, up), (off, o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count  hate_speech  offensive_language  neither  class  \\\n",
       "id                                                           \n",
       "0       3            0                   0        3      2   \n",
       "1       3            0                   3        0      1   \n",
       "2       3            0                   3        0      1   \n",
       "3       3            0                   2        1      1   \n",
       "4       6            0                   6        0      1   \n",
       "5       3            1                   2        0      1   \n",
       "6       3            0                   3        0      1   \n",
       "7       3            0                   3        0      1   \n",
       "8       3            0                   3        0      1   \n",
       "9       3            1                   2        0      1   \n",
       "10      3            0                   3        0      1   \n",
       "11      3            0                   3        0      1   \n",
       "12      3            0                   2        1      1   \n",
       "13      3            0                   3        0      1   \n",
       "14      3            1                   2        0      1   \n",
       "\n",
       "                                            raw_tweet  \\\n",
       "id                                                      \n",
       "0   !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1   !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2   !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "3   !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4   !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "5   !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...   \n",
       "6   !!!!!!\"@__BrighterDays: I can not just sit up ...   \n",
       "7   !!!!&#8220;@selfiequeenbri: cause I'm tired of...   \n",
       "8   \" &amp; you might not get ya bitch back &amp; ...   \n",
       "9   \" @rhythmixx_ :hobbies include: fighting Maria...   \n",
       "10  \" Keeks is a bitch she curves everyone \" lol I...   \n",
       "11                 \" Murda Gang bitch its Gang Land \"   \n",
       "12  \" So hoes that smoke are losers ? \" yea ... go...   \n",
       "13      \" bad bitches is the only thing that i like \"   \n",
       "14                            \" bitch get up off me \"   \n",
       "\n",
       "                                     pos_tagged_tweet  \\\n",
       "id                                                      \n",
       "0   [(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...   \n",
       "1   [(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...   \n",
       "2   [(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...   \n",
       "3   [(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...   \n",
       "4   [(!, .), (!, .), (!, .), (rt, NOUN), (mentionh...   \n",
       "5   [(!, .), (!, .), (!, .), (mentionhere, NOUN), ...   \n",
       "6   [(!, .), (!, .), (!, .), (mentionhere, NOUN), ...   \n",
       "7   [(!, .), (!, .), (!, .), (mentionhere, ADV), (...   \n",
       "8   [(&, CONJ), (you, PRON), (might, VERB), (not, ...   \n",
       "9   [(mentionhere, ADV), (:, .), (hobbies, NOUN), ...   \n",
       "10  [(keeks, NOUN), (is, VERB), (a, DET), (bitch, ...   \n",
       "11  [(murda, NOUN), (gang, NOUN), (bitch, VERB), (...   \n",
       "12  [(so, ADV), (hoes, VERB), (that, ADP), (smoke,...   \n",
       "13  [(bad, ADJ), (bitches, NOUN), (is, VERB), (the...   \n",
       "14  [(bitch, NOUN), (get, VERB), (up, PRT), (off, ...   \n",
       "\n",
       "                                     lemmatized_tweet  \n",
       "id                                                     \n",
       "0   [(!, !), (!, !), (!, !), (rt, rt), (mentionher...  \n",
       "1   [(!, !), (!, !), (!, !), (rt, rt), (mentionher...  \n",
       "2   [(!, !), (!, !), (!, !), (rt, rt), (mentionher...  \n",
       "3   [(!, !), (!, !), (!, !), (rt, rt), (mentionher...  \n",
       "4   [(!, !), (!, !), (!, !), (rt, rt), (mentionher...  \n",
       "5   [(!, !), (!, !), (!, !), (mentionhere, mention...  \n",
       "6   [(!, !), (!, !), (!, !), (mentionhere, mention...  \n",
       "7   [(!, !), (!, !), (!, !), (mentionhere, mention...  \n",
       "8   [(&, &), (you, you), (might, might), (not, not...  \n",
       "9   [(mentionhere, mentionhere), (:, :), (hobbies,...  \n",
       "10  [(keeks, keeks), (is, be), (a, a), (bitch, bit...  \n",
       "11  [(murda, murda), (gang, gang), (bitch, bitch),...  \n",
       "12  [(so, so), (hoes, hoe), (that, that), (smoke, ...  \n",
       "13  [(bad, bad), (bitches, bitch), (is, be), (the,...  \n",
       "14  [(bitch, bitch), (get, get), (up, up), (off, o...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/labeled_data_preprocessed.csv\", index_col = 0)\n",
    "annotation_re = re.compile(r\"(.+)/(.+)\")\n",
    "df['pos_tagged_tweet'] = df['pos_tagged_tweet'].apply(lambda x: [annotation_re.findall(s)[0] for s in x.split()])\n",
    "df['lemmatized_tweet'] = df['lemmatized_tweet'].apply(lambda x: [annotation_re.findall(s)[0] for s in x.split()])\n",
    "df.head(15)  # the first 25 tweets, along with their annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legend: Columns keys\n",
    "\n",
    "**`count`** = number of human annotators who coded each tweet (min is 3).\n",
    "\n",
    "**`hate_speech`** = number of annotators who judged the tweet to be hate speech.\n",
    "\n",
    "**`offensive_language`** = number of annotators who judged the tweet to be offensive.\n",
    "\n",
    "**`neither`** = number of annotators who judged the tweet to be neither offensive nor non-offensive.\n",
    "\n",
    "**`class`** = class label for majority of annotators (**this is our target label**).\n",
    "\n",
    "    0 - hate speech\n",
    "    1 - offensive language\n",
    "    2 - neither\n",
    "   \n",
    "**`raw_tweet`** = raw tweet text (very noisy, it should be used only for inspection and/or to run a processing pipeline from scratch)\n",
    "\n",
    "**`pos_tagged_tweet`** = an annotated version of each tweet obtained by:\n",
    "\n",
    "    - cleaning the tweet (removing unwanted characters and lowering the text)\n",
    "    - replacing mentions, hashtags and url with the \"MENTIONHERE\", \"HASHTAGHERE\" and \"URLHERE\" tag, respectively\n",
    "    - tokenizing the text (roughly, splitting the into morphological units such as words and punctuation marks)\n",
    "    - annotating the grammatical category (a.k.a. Part of Speech: PoS) of each token\n",
    "\n",
    "**`lemmatized_tweet`** = annotated version of each tweet obtained by using the annotation available in the `pos_tagged_tweet` representation in order to tag the base form (e.g. \"*be*\" for the inflected word \"*am*\") of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging notation\n",
    "\n",
    "The pos-tagged and the lemmatized tweets have been represented using the [nltk notation](http://www.nltk.org/book_1ed/ch05.html), where \n",
    "\n",
    "- each **tweet** is represented as a list of annotated tokens\n",
    "\n",
    "- each **annotated token** is represented as a `(attested word, tag)` tuple\n",
    "\n",
    "For instance, we can verbalize the following PoS annotation as:\n",
    "\n",
    "> *\"the article \"the\" is followed by the adjective \"grand\", in turn followed by the noun \"jury\" and the verb \"commented\"*\n",
    "\n",
    "`[('The', 'DET'), ('grand', 'ADJ'), ('jury', 'NOUN'), ('commented', 'VERB')]`\n",
    "\n",
    "... while the following lemmatized text can be read as \n",
    "\n",
    "> *the text is composed by inflected forms of the lemmas \"the\", \"grand\", \"jury\" and \"comment\"* (note that the only token that is different from its base form is the latter)\n",
    "\n",
    "`[('The', 'The'), ('grand', 'grand'), ('jury', 'jury'), ('commented', 'comment')]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech TagSet\n",
    "\n",
    "For PoS-tagging our tweets we've used the Universal tagset proposed by [Petrov et al (2011)](https://arxiv.org/abs/1104.2086):\n",
    "\n",
    "|  tag  | meaning |\n",
    "|:-----:|:---------|\n",
    "| VERB | verbs (all tenses and modes) |\n",
    "| NOUN | nouns (common and proper) |\n",
    "| PRON | pronouns |\n",
    "| ADJ | adjectives |\n",
    "| ADV | adverbs |\n",
    "| ADP | adpositions (prepositions and postpositions) |\n",
    "| CONJ | conjunctions |\n",
    "| DET | determiners |\n",
    "| NUM | cardinal numbers |\n",
    "| PRT | particles or other function words |\n",
    "| X | other: foreign words, typos, abbreviations |\n",
    "| \\. | punctuation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['count', 'hate_speech', 'offensive_language', 'neither', 'class', 'raw_tweet', 'pos_tagged_tweet', 'lemmatized_tweet'])\n"
     ]
    }
   ],
   "source": [
    "# note that the keys of the outmost dictionary are the column names\n",
    "print(df_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "# while the keys of each embedded dictionary are the rows indices\n",
    "print(list(df_dict['raw_tweet'].keys())[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "In this step, we'll extract the features that we will later used to convert each tweet into a vector. Let's store all the estimates as subdictionaries of a `features_dictionary` dictionary of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dictionary = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of tokens in each tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 26, 35, 20, 43, 53, 31, 30, 17, 11]\n"
     ]
    }
   ],
   "source": [
    "rawList = df_dict['raw_tweet']\n",
    "token_counts = dict()\n",
    "\n",
    "for x in rawList.keys():\n",
    "    tweet = rawList[x]\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    token_counts[x] = len(tokens)\n",
    "    \n",
    "features_dictionary[\"tokens_counts\"] = token_counts\n",
    "\n",
    "print(list(features_dictionary[\"tokens_counts\"].values())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of characters in each tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('!', '!'), ('!', '!'), ('!', '!'), ('rt', 'rt'), ('mentionhere', 'mentionhere'), (':', ':'), ('as', 'a'), ('a', 'a'), ('woman', 'woman'), ('you', 'you'), (\"shouldn't\", \"shouldn't\"), ('complain', 'complain'), ('about', 'about'), ('cleaning', 'clean'), ('up', 'up'), ('your', 'your'), ('house', 'house'), ('.', '.'), ('&', '&'), ('as', 'a'), ('a', 'a'), ('man', 'man'), ('you', 'you'), ('should', 'should'), ('always', 'always'), ('take', 'take'), ('the', 'the'), ('trash', 'trash'), ('out', 'out'), ('...', '...')]]\n",
      "[110, 71, 90, 46, 90, 95, 77, 67, 36, 46]\n"
     ]
    }
   ],
   "source": [
    "tweetList = df_dict['lemmatized_tweet']\n",
    "character_counts = dict()\n",
    "\n",
    "for x in tweetList.keys():\n",
    "    charCount = 0\n",
    "    for orig,lem in tweetList[x]:\n",
    "        charCount += len(orig)\n",
    "    character_counts[x] = charCount\n",
    "    \n",
    "features_dictionary[\"characters_counts\"] = character_counts\n",
    "\n",
    "print(list(df_dict['lemmatized_tweet'].values())[:1])\n",
    "print(list(features_dictionary[\"characters_counts\"].values())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Count indicators for hashtags, mentions, and URLs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('keeks', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('bitch', 'NOUN'), ('she', 'PRON'), ('curves', 'VERB'), ('everyone', 'NOUN'), ('lol', 'NOUN'), ('i', 'NOUN'), ('walked', 'VERB'), ('into', 'ADP'), ('a', 'DET'), ('conversation', 'NOUN'), ('like', 'ADP'), ('this', 'DET'), ('.', '.'), ('smh', 'NOUN')], [('murda', 'NOUN'), ('gang', 'NOUN'), ('bitch', 'VERB'), ('its', 'PRON'), ('gang', 'NOUN'), ('land', 'NOUN')], [('so', 'ADV'), ('hoes', 'VERB'), ('that', 'ADP'), ('smoke', 'NOUN'), ('are', 'VERB'), ('losers', 'NOUN'), ('?', '.'), ('yea', 'NOUN'), ('...', '.'), ('go', 'VERB'), ('on', 'ADP'), ('ig', 'NOUN')], [('bad', 'ADJ'), ('bitches', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('only', 'ADJ'), ('thing', 'NOUN'), ('that', 'ADP'), ('i', 'NOUN'), ('like', 'ADP')], [('bitch', 'NOUN'), ('get', 'VERB'), ('up', 'PRT'), ('off', 'ADP'), ('me', 'PRON')], [('bitch', 'NOUN'), ('nigga', 'NOUN'), ('miss', 'VERB'), ('me', 'PRON'), ('with', 'ADP'), ('it', 'PRON')], [('bitch', 'NOUN'), ('plz', 'NOUN'), ('whatever', 'DET')], [('bitch', 'NOUN'), ('who', 'PRON'), ('do', 'VERB'), ('you', 'PRON'), ('love', 'VERB')], [('bitches', 'NOUN'), ('get', 'VERB'), ('cut', 'VERB'), ('off', 'PRT'), ('everyday', 'ADJ'), ('b', 'NOUN')], [('black', 'ADJ'), ('bottle', 'NOUN'), ('&', 'CONJ'), ('a', 'DET'), ('bad', 'ADJ'), ('bitch', 'NOUN')]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "num_hashtags = dict()\n",
    "num_mentions = dict()\n",
    "num_urls = dict()\n",
    "\n",
    "for x in tweetList.keys():\n",
    "    hashCount = 0\n",
    "    mentCount = 0\n",
    "    urlsCount = 0\n",
    "    for orig,lem in tweetList[x]:\n",
    "        if orig == 'hashtaghere':\n",
    "            hashCount += 1\n",
    "        elif orig == 'mentionhere':\n",
    "            mentCount += 1\n",
    "        elif orig == 'urlhere':\n",
    "            urlsCount += 1\n",
    "    num_hashtags[x] = hashCount\n",
    "    num_mentions[x] = mentCount\n",
    "    num_urls[x] = urlsCount\n",
    "    \n",
    "features_dictionary[\"num_hashtags\"] = num_hashtags\n",
    "features_dictionary[\"num_mentions\"] = num_mentions\n",
    "features_dictionary[\"num_urls\"] = num_urls\n",
    "\n",
    "print(list(df_dict['pos_tagged_tweet'].values())[10:20])\n",
    "print(list(features_dictionary[\"num_hashtags\"].values())[:20])\n",
    "print(list(features_dictionary[\"num_mentions\"].values())[:20])\n",
    "print(list(features_dictionary[\"num_urls\"].values())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Binary indicator for retweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[('!', '.'), ('!', '.'), ('!', '.'), ('rt', 'NOUN'), ('mentionhere', 'ADV'), (':', '.'), ('as', 'ADP'), ('a', 'DET'), ('woman', 'NOUN'), ('you', 'PRON'), (\"shouldn't\", 'VERB'), ('complain', 'VERB'), ('about', 'ADP'), ('cleaning', 'VERB'), ('up', 'PRT'), ('your', 'PRON'), ('house', 'NOUN'), ('.', '.'), ('&', 'CONJ'), ('as', 'ADP'), ('a', 'DET'), ('man', 'NOUN'), ('you', 'PRON'), ('should', 'VERB'), ('always', 'ADV'), ('take', 'VERB'), ('the', 'DET'), ('trash', 'NOUN'), ('out', 'ADP'), ('...', '.')]]\n"
     ]
    }
   ],
   "source": [
    "is_retweet = dict()\n",
    "\n",
    "for x in tweetList.keys():\n",
    "    isRetweet = 0\n",
    "    for orig,lem in tweetList[x]:\n",
    "        if orig == 'rt':\n",
    "            isRetweet = 1\n",
    "    is_retweet[x] = isRetweet\n",
    "    \n",
    "features_dictionary[\"is_retweet\"] = is_retweet\n",
    "\n",
    "print(list(features_dictionary[\"is_retweet\"].values())[:10])\n",
    "print(list(df_dict[\"pos_tagged_tweet\"].values())[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of emoticons in each tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 1, 0, 2, 0, 0]\n",
      "[\"!!!!&#8220;@selfiequeenbri: cause I'm tired of you big bitches coming for us skinny girls!!&#8221;\"]\n"
     ]
    }
   ],
   "source": [
    "emoticons_count = dict()\n",
    "\n",
    "for x in rawList.keys():\n",
    "    numEmoticon = 0\n",
    "    tweet = rawList[x]\n",
    "    words = tweet.split(\" \")\n",
    "    for word in words:\n",
    "        if re.search(r\"&#\\w+;\", word):\n",
    "            numEmoticon += 1\n",
    "    emoticons_count[x] = numEmoticon\n",
    "    \n",
    "features_dictionary[\"emoticons_count\"] = emoticons_count\n",
    "\n",
    "print(list(features_dictionary[\"emoticons_count\"].values())[:10])\n",
    "print(list(df_dict[\"raw_tweet\"].values())[7:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BOW features**\n",
    "\n",
    "The `data/hatebase.json` file contains the original lexicon from `Hatebase.org` that was used to sample tweets. \n",
    "\n",
    "Counting occurences of significant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/hatebase.json\", \"r\", encoding=\"utf8\") as infile:\n",
    "    hatebase = set(json.loads(infile.read()))\n",
    "    \n",
    "hatebase_frequencies = dict()\n",
    "\n",
    "for sigword in hatebase:\n",
    "    tweet_occurences = dict()\n",
    "    occurences = 0\n",
    "    for x in tweetList.keys():\n",
    "        for orig,tword in tweetList[x]:\n",
    "            if tword == sigword:\n",
    "                occurences += 1\n",
    "        tweet_occurences[x] = occurences\n",
    "    \n",
    "    hatebase_frequencies[sigword] = tweet_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **n-grams features**\n",
    "\n",
    "The `data/refined_ngram.json` file contains a manually selected lexicon of *n-grams* (i.e. a contiguous sequence of *n* words), where $1 < n < 5$.\n",
    "\n",
    "Counting occurences of significant n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of white  :  21970 3\n",
      "dykes  :  16999 2\n",
      "fags  :  6135 2\n",
      "white trash  :  10799 2\n",
      "faggot  :  5323 2\n",
      "faggot  :  8587 2\n",
      "nigger  :  4442 2\n",
      "nigger  :  4866 2\n",
      "spic  :  808 4\n",
      "a nigger  :  4866 2\n",
      "white person  :  11824 2\n",
      "full of white  :  21970 3\n",
      "is full of white  :  21970 2\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/refined_ngram.json\", \"r\", encoding=\"utf8\") as infile:\n",
    "    refined_ngrams = set(json.loads(infile.read()))\n",
    "    \n",
    "ngrams_frequencies = dict()\n",
    "\n",
    "for ngram in refined_ngrams:\n",
    "    ngram_occurences = dict()\n",
    "    ngramlen = len(ngram.split(\" \"))\n",
    "    for x in rawList.keys():\n",
    "        occurences = 0\n",
    "        words = rawList[x].split(\" \")\n",
    "        for gramList in list(nltk.ngrams(words,ngramlen)):\n",
    "            tweetgram = ' '.join(gramList)\n",
    "            if tweetgram == ngram:\n",
    "                occurences += 1\n",
    "                \n",
    "        ngram_occurences[x] = occurences\n",
    "        \n",
    "        if occurences > 1:\n",
    "            print(ngram, \" : \", x, occurences)\n",
    "    ngrams_frequencies[ngram] = ngram_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train models, dictionaries are converted into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids = sorted(df_dict['raw_tweet'].keys())\n",
    "other_features_names = sorted(features_dictionary.keys())\n",
    "\n",
    "# let's create a matrix of zeros and populate it by iterating over our dictionary \n",
    "other_features_matrix = np.zeros((len(tweet_ids), len(other_features_names)))\n",
    "\n",
    "for fid, feature in enumerate(other_features_names):\n",
    "    for tid, tweet in enumerate(tweet_ids):\n",
    "        other_features_matrix[tid, fid] = features_dictionary[feature][tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "hatebase_words = sorted(hatebase_frequencies.keys())\n",
    "\n",
    "hatebase_matrix = np.zeros((len(tweet_ids),len(hatebase_words)))\n",
    "\n",
    "for hid, word in enumerate(hatebase_words):\n",
    "    for tid, tweet in enumerate(tweet_ids):\n",
    "        hatebase_matrix[tid,hid] = hatebase_frequencies[word][tweet]\n",
    "        \n",
    "print(hatebase_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_grams = sorted(ngrams_frequencies.keys())\n",
    "\n",
    "ngrams_matrix = np.zeros((len(tweet_ids),len(ngrams_grams)))\n",
    "\n",
    "for nid, ngram in enumerate(ngrams_grams):\n",
    "    for tid, tweet in enumerate(tweet_ids):\n",
    "        ngrams_matrix[tid,nid] = ngrams_frequencies[ngram][tweet]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing features, using tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tf-idf}_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t = \\text{tf}_{t,d} \\times log \\left( \\frac{N}{\\text{df}_t}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- where:\n",
    "    - tf$_{t,d}$ is the number of occurrence of $t$ in $d$ (the absolute or relative frequency)\n",
    "    - $N$ is the total number of documents\n",
    "    - df$_t$ is the number of documents in which $t$ appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 2 1]\n",
      " [0 2 0 0 1 0]\n",
      " [1 0 0 1 0 0]\n",
      " [2 2 0 1 1 1]]\n",
      "\n",
      "[[0.17235756 0.         0.27003143 0.         0.34471513 0.21289588]\n",
      " [0.         0.7118486  0.         0.         0.2881514  0.        ]\n",
      " [0.44738747 0.         0.         0.55261253 0.         0.        ]\n",
      " [0.25186393 0.31110206 0.         0.15555103 0.12593196 0.15555103]]\n"
     ]
    }
   ],
   "source": [
    "# a dummy tweets (rows) x words (columns) matrix\n",
    "dummy_matrix = np.random.randint(3, size=(4, 6))\n",
    "print(dummy_matrix, end = \"\\n\\n\")\n",
    "\n",
    "# let's weight our matrix (norm=\"l1\" because we chose to adjust the term frequency for document length)\n",
    "tfidf_matrix = TfidfTransformer(norm=\"l1\").fit_transform(dummy_matrix).todense()\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "hatebase_matrix_tfidf = TfidfTransformer(norm=\"l1\").fit_transform(hatebase_matrix).todense()\n",
    "\n",
    "ngrams_matrix_tfidf = TfidfTransformer(norm=\"l1\").fit_transform(ngrams_matrix).todense()\n",
    "\n",
    "print(hatebase_matrix_tfidf)\n",
    "print(ngrams_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging our feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24729\n"
     ]
    }
   ],
   "source": [
    "bow_features_set = np.concatenate((other_features_matrix, hatebase_matrix_tfidf), axis=1)\n",
    "ngrams_features_set = np.concatenate((other_features_matrix, ngrams_matrix_tfidf), axis=1)\n",
    "\n",
    "print(len(bow_features_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "Logistic regression and support vector machine are compared on the dataset, using the different feature sets.\n",
    "\n",
    "- Logistic Regression with the `bow_features_set`\n",
    "\n",
    "- Logistic Regression with the `ngrams_features_set`\n",
    "\n",
    "- Support Vector Machine with the `bow_features_set`\n",
    "\n",
    "- Support Vector Machine with the `ngrams_features_set`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development and Test Set\n",
    "\n",
    "The dataset is divided into a development (part training) and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2473\n",
      "22256\n",
      "24729\n",
      "24729\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "tweet_num = len(df_dict['raw_tweet'].keys())\n",
    "\n",
    "id_list = random.sample(range(1, tweet_num+1), tweet_num)\n",
    "bound = int(tweet_num*0.9)\n",
    "\n",
    "dev_set = id_list[:bound]\n",
    "test_set = id_list[bound:]\n",
    "# set contains of ID nrs of tweets\n",
    "\n",
    "print(len(test_set))\n",
    "print(len(dev_set))\n",
    "print(tweet_num)\n",
    "print(len(test_set) + len(dev_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing our ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we estimate the precision, recall and f-measure of the models by comparing `y_pred` (the predicted labels) and `y_test` (the gold standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold cross validation\n",
    "\n",
    "The development set is divided into subsets called folds. For each fold, the model is trained on all except that fold. Then it is tested on that fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "To evaluate the quality of our model we peform the annotation of the test set (for which we have the correct annotation) and we calculate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**: the percentage of inputs documents correctly classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision, Recall and F-measure** are calculated from the following estimates:\n",
    "\n",
    "- True Positives: of items that we correctly identified as relevant.\n",
    "- True Negatives: items that we correctly identified as irrelevant.\n",
    "- False Positives (or Type I errors): irrelevant items that we incorrectly identified as relevant.\n",
    "- False Negatives (or Type II errors): relevant items that we incorrectly identified as irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![True and False Positives and Negatives](https://www.nltk.org/images/precision-recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Precision**: how many of the items that we identified were relevant: $\\frac{\\text{TP}}{(\\text{TP}+\\text{FP})}$.\n",
    "\n",
    "\n",
    "- **Recall**: how many of the relevant items that we identified, is $\\frac{\\text{TP}}{(\\text{TP}+\\text{FN})}$,\n",
    "\n",
    "\n",
    "- **F$_1$-measure**: the harmonic mean of the precision and recall: $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_matrix = list(df_dict['class'].values())\n",
    "\n",
    "def log_reg(X_train,y_train,X_test):\n",
    "    # Create logistic regression object\n",
    "    regr = linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=10000)\n",
    "\n",
    "    print(\"train... logistic regression\")\n",
    "    #Train the model using the training sets\n",
    "    regr.fit(X_train, y_train)\n",
    "    #Predict the response for test dataset\n",
    "    return(regr.predict(X_test))\n",
    "\n",
    "def svmachine(X_train,y_train,X_test):\n",
    "    clf = svm.LinearSVC(max_iter=10000)\n",
    "\n",
    "    print(\"train... SVM\")\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(X_train, y_train)\n",
    "    #Predict the response for test dataset\n",
    "    return(clf.predict(X_test))\n",
    "\n",
    "\n",
    "def calculations(predY, realY):\n",
    "    TP0=TP1=TP2=FP0=FP1=FP2=FN0=FN1=FN2=TN0=TN1=TN2 = 0\n",
    "    for x in range(0,len(predY)): # gaat alle y waardes af\n",
    "        if (predY[x] == 0 and realY[x] == 0): \n",
    "            TP0 += 1\n",
    "        if (predY[x] == 1 and realY[x] == 1): \n",
    "            TP1 += 1\n",
    "        if (predY[x] == 2 and realY[x] == 2): \n",
    "            TP2 += 1\n",
    "        if (predY[x] == 0 and realY[x] != 0):\n",
    "            FP0 += 1\n",
    "        if (predY[x] == 1 and realY[x] != 1):\n",
    "            FP1 += 1\n",
    "        if (predY[x] == 2 and realY[x] != 2):\n",
    "            FP2 += 1\n",
    "        if (predY[x] != 0 and realY[x] == 0):\n",
    "            FN0 += 1\n",
    "        if (predY[x] != 1 and realY[x] == 1):\n",
    "            FN1 += 1\n",
    "        if (predY[x] != 2 and realY[x] == 2):\n",
    "            FN2 += 1\n",
    "        if (predY[x] != 0 and realY[x] != 0):\n",
    "            TN0 += 1\n",
    "        if (predY[x] != 1 and realY[x] != 1):\n",
    "            TN1 += 1\n",
    "        if (predY[x] != 2 and realY[x] != 2):\n",
    "            TN2 += 1\n",
    "        \n",
    "    # RECALL\n",
    "    recall0 = (TP0 / (TP0 + FN0))\n",
    "    recall1 = (TP1 / (TP1 + FN1))\n",
    "    recall2 = (TP2 / (TP2 + FN2))    \n",
    "    recall = ((recall0+recall1+recall2)/3)\n",
    "\n",
    "    #PRECISION\n",
    "    if (TP0 + FP0) ==0: # avoid division by 0.\n",
    "        precision0 = 0\n",
    "    else:\n",
    "        precision0 = ( TP0 / (TP0 + FP0) )\n",
    "        #print(\"training \", i, \" precision0: \", precision0)\n",
    "    if (TP1 + FP1) ==0: \n",
    "        precision1 = 0\n",
    "    else:\n",
    "         precision1 = ( TP1 / (TP1 + FP1) )   \n",
    "        #print(\"training \", i, \" precision1: \", precision1)\n",
    "    if (TP2 + FP2) ==0: \n",
    "        precision2 = 0\n",
    "    else:\n",
    "        precision2 = ( TP2 / (TP2 + FP2) )   \n",
    "        #print(\"training \", i, \" precision2: \", precision2)        \n",
    "    precision = ((precision0+precision1+precision2)/3)\n",
    "        \n",
    "    #F_MEASURE\n",
    "    if precision0 == 0 and recall0 == 0:\n",
    "        f_measure0 = 0\n",
    "    else:\n",
    "        f_measure0 = 2 * ((precision0 * recall0) / (precision0 + recall0))\n",
    "    if precision1 == 0 and recall1 == 0:\n",
    "        f_measure1 = 0\n",
    "    else:\n",
    "        f_measure1 = 2 * ((precision1 * recall1) / (precision1 + recall1))\n",
    "    if precision2 == 0 and recall2 == 0:\n",
    "        f_measure2 = 0\n",
    "    else:\n",
    "        f_measure2 = 2 * ((precision2 * recall2) / (precision2 + recall2))  \n",
    "    f_measure = ((f_measure0+f_measure1+f_measure2)/3)\n",
    "    \n",
    "    #ACCURACY\n",
    "    accuracy = ((TP0+TP1+TP2 + TN0+TN1+TN2) / (TP0+TP1+TP2 + TN0+TN1+TN2 + FP0+FP1+FP2 + FN0+FN1+FN2))\n",
    "    return TP0, TP1, TP2, FP0, FP1, FP2, FN0, FN1, FN2, TN0, TN1, TN2, recall, precision, f_measure, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### logistic regression, ngrams ###\n",
      "starting...\n",
      "train... logistic regression\n",
      "train... logistic regression\n",
      "train... logistic regression\n",
      "train... logistic regression\n",
      "train... logistic regression\n",
      "CROSS VALIDATION TRAINING DONE...\n",
      "average recall:  0.37187233048233886\n",
      "average precision:  0.6063129529655206\n",
      "average f_measure:  0.36819849891327816\n",
      "average accuracy:  0.8528936866639227\n",
      "### SVM, ngrams ####\n",
      "starting...\n",
      "train... SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION TRAINING DONE...\n",
      "average recall:  0.39856737637252077\n",
      "average precision:  0.639168655767081\n",
      "average f_measure:  0.4024328518307792\n",
      "average accuracy:  0.8505270225070312\n",
      "### logistic regression, BOW ###\n",
      "starting...\n",
      "train... logistic regression\n",
      "train... logistic regression\n",
      "train... logistic regression\n",
      "train... logistic regression\n",
      "train... logistic regression\n",
      "CROSS VALIDATION TRAINING DONE...\n",
      "average recall:  0.35590803225259793\n",
      "average precision:  0.462052434582189\n",
      "average f_measure:  0.3372882143132313\n",
      "average accuracy:  0.8536724369291144\n",
      "### SVM, BOW ###\n",
      "starting...\n",
      "train... SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... SVM\n",
      "CROSS VALIDATION TRAINING DONE...\n",
      "average recall:  0.36008996770190127\n",
      "average precision:  0.4662311224783185\n",
      "average f_measure:  0.3402510696350123\n",
      "average accuracy:  0.8523847069507786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "def create_folds(dev): #splits dev_folds in 5 \n",
    "    dev_set_split = np.array_split(dev, 5)\n",
    "    return dev_set_split\n",
    "\n",
    "def evaluate_train(algorithm, features,dev): #(welk algoritme, bow_features_set, dev_set )\n",
    "    print(\"starting...\", )\n",
    "    \n",
    "    dev_shuffled = random.shuffle(dev) #dev_set is shuffled\n",
    "    dev_folds = create_folds(dev) #folds are created, data is split up\n",
    "    \n",
    "    recall = 0\n",
    "    precision = 0\n",
    "    accuracy = 0 \n",
    "    f_measure = 0\n",
    "    totrecall = 0 #total recall \n",
    "    totprecision = 0\n",
    "    totaccuracy = 0\n",
    "    totf_measure = 0 \n",
    "    true_positives = 0 #total true positives\n",
    "    false_positives = 0 #total false positives\n",
    "    false_negatives = 0 #total false negatives\n",
    "    true_negatives = 0 #total true negatives\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        devtraining = np.concatenate(np.delete(dev_folds, i, axis=0))\n",
    "        #fold is deleted from devtraining\n",
    "        devtest = np.take(dev_folds, i, axis=0)\n",
    "        #testdata\n",
    "\n",
    "        # X for the training set for this fold\n",
    "        fold_X_train = np.take(features, devtraining-1,axis=0)\n",
    "        # X for the test set (=this fold)\n",
    "        fold_X_test = np.take(features, devtest-1, axis=0)\n",
    "        # y for the training set for this fold\n",
    "        fold_y_train = np.take(y_matrix, devtraining-1)\n",
    "        # y for the test set (=this fold)\n",
    "        fold_y_test = np.take(y_matrix, devtest-1)\n",
    "        \n",
    "        if algorithm == svmachine:\n",
    "            fold_y_pred = svmachine(fold_X_train,fold_y_train,fold_X_test)\n",
    "        if algorithm == log_reg:\n",
    "            fold_y_pred = log_reg(fold_X_train,fold_y_train, fold_X_test)\n",
    "        \n",
    "        TP0, TP1, TP2, FP0, FP1, FP2, FN0, FN1, FN2, TN0, TN1, TN2, recall, precision, f_measure, accuracy = calculations(fold_y_pred, fold_y_test)\n",
    "      \n",
    "        true_positives += TP0 + TP1 + TP2\n",
    "        false_positives += FP0 + FP1 + FP2\n",
    "        false_negatives += FN0 + FN1 + FN2\n",
    "        true_negatives += TN0 + TN1 + TN2\n",
    "        \n",
    "        # RECALL\n",
    "        totrecall += recall\n",
    "\n",
    "        #PRECISION        \n",
    "        totprecision += precision \n",
    "        \n",
    "        #F_MEASURE\n",
    "        totf_measure += f_measure\n",
    "        \n",
    "        #ACCURACY\n",
    "        totaccuracy += accuracy\n",
    "    \n",
    "        \n",
    "    #cross validation average:\n",
    "    print(\"CROSS VALIDATION TRAINING DONE...\")\n",
    "    \n",
    "    totrecall = totrecall / len(dev_folds)\n",
    "    totprecision = totprecision / len(dev_folds)\n",
    "    totf_measure = totf_measure / len(dev_folds)\n",
    "    totaccuracy = totaccuracy / len(dev_folds)\n",
    "    \n",
    "    print(\"average recall: \", totrecall)\n",
    "    print(\"average precision: \", totprecision)\n",
    "    print(\"average f_measure: \", totf_measure)\n",
    "    print(\"average accuracy: \", totaccuracy)\n",
    "    \n",
    "    return totrecall,totprecision,totf_measure,totaccuracy,true_positives,false_positives,false_negatives,true_negatives\n",
    "\n",
    "#REG NGRAMS\n",
    "print(\"### logistic regression, ngrams ###\")\n",
    "uitkomst_reg_ngrams = evaluate_train(log_reg, ngrams_features_set,dev_set)\n",
    "#average recall:  0.3792066626775771\n",
    "#average precision:  0.6057187244358799\n",
    "#average f_measure:  0.3807514820254992\n",
    "#average accuracy:  0.8528036846460096\n",
    "\n",
    "#SVM NGRAMS\n",
    "print(\"### SVM, ngrams ####\")\n",
    "uitkomst_svm_ngrams = evaluate_train(svmachine, ngrams_features_set,dev_set)\n",
    "#average recall:  0.41573966151298913\n",
    "#average precision:  0.5859251810163755\n",
    "#average f_measure:  0.4225343666916575\n",
    "#average accuracy:  0.8391750268085033\n",
    "\n",
    "#REG BOW\n",
    "print(\"### logistic regression, BOW ###\")\n",
    "uitkomst_reg_bow = evaluate_train(log_reg, bow_features_set,dev_set)\n",
    "#average recall:  0.3572884464813689\n",
    "#average precision:  0.4658980735705338\n",
    "#average f_measure:  0.3396715342122703\n",
    "#average accuracy:  0.8536123772691345\n",
    "\n",
    "#SVM BOW\n",
    "print(\"### SVM, BOW ###\")\n",
    "uitkomst_svm_bow = evaluate_train(svmachine, bow_features_set,dev_set)\n",
    "#average recall:  0.3728969317871915\n",
    "#average precision:  0.4586349157751292\n",
    "#average f_measure:  0.34848709197898986\n",
    "#average accuracy:  0.8368076292320581\n",
    "\n",
    "#logistic regression with BOW features has the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best performing model\n",
    "\n",
    "Now, the models are tested. This is done by using the entire development set as a training set. Then the resulting models are trained on the remaining 10% of tweets that haven't been used yet, the test set.\n",
    "\n",
    "The model is compared to the human annotations and the following are manually calculated:\n",
    "\n",
    "- its accuracy\n",
    "\n",
    "- its precision, recall and f-measure\n",
    "\n",
    "- the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... logistic regression\n",
      "RECALL:  0.3557339345365656\n",
      "PRECISION:  0.47621292741519045\n",
      "F_MEASURE:  0.335102911248949\n",
      "ACCURACY:  0.8487666801455722\n",
      "CONFUSION MATRIX:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_model(algorithm, features, dev, test):\n",
    "    \n",
    "    dev2 = np.array(dev)\n",
    "    test2 = np.array(test)\n",
    "    \n",
    "    # X for the training (90%)\n",
    "    X_train = np.take(features, dev2-1,axis=0)\n",
    "    # y for the training (90%)\n",
    "    y_train = np.take(y_matrix, dev2-1, axis=0)\n",
    "    \n",
    "    # X for the test set (10%)\n",
    "    X_test = np.take(features, test2-1, axis=0)\n",
    "    # y for the test set (10%)\n",
    "    y_test = np.take(y_matrix, test2-1, axis=0)\n",
    "    \n",
    "    evaluation_y = algorithm(X_train,y_train,X_test)\n",
    "    TP0, TP1, TP2, FP0, FP1, FP2, FN0, FN1, FN2, TN0, TN1, TN2, recall, precision, f_measure, accuracy = calculations(evaluation_y,y_test)\n",
    "    \n",
    "    TP = TP0+TP1+TP2\n",
    "    FP = FP0+FP1+FP2\n",
    "    FN = FN0+FN1+FN2\n",
    "    TN = TN0+TN1+TN2\n",
    "    \n",
    "    print(\"RECALL: \", recall)\n",
    "    print(\"PRECISION: \", precision)\n",
    "    print(\"F_MEASURE: \", f_measure)\n",
    "    print(\"ACCURACY: \", accuracy)\n",
    "    print(\"CONFUSION MATRIX:\")\n",
    "    confusionmatrix = np.asarray([[TP, FP], [FN, TN]])\n",
    "    confusionmatrix2 = np.asarray([[(TP0+TN1+TN2),(FP0+FN1+TN2),(FP0+FN2+TN1)],\n",
    "                                  [(FP1+FN0+TN2),(TN2+TP1+TN0),(FP1+FN2+TN0)],\n",
    "                                  [(FP2+FN0+TN1),(FP2+FN1+TN0),(TP1+TN0+TN1)]])\n",
    "    \n",
    "    return(confusionmatrix, confusionmatrix2)\n",
    "cm, cm2 = test_model(log_reg, bow_features_set, dev_set, test_set) \n",
    "plt.imshow(cm)\n",
    "plt.imshow(cm2)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the model is tested on the tweets on which there was no substantial agreement. Most of these are counted as offensive language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_agreement = df[df[[\"hate_speech\", \"offensive_language\", \"neither\"]].gt(0).sum(axis=1) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      263\n",
       "1    14302\n",
       "2     2872\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_high_agreement['class'].value_counts(sort = False).plot.bar(color='#1f77b4')\n",
    "df_high_agreement['class'].value_counts(sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17437\n",
      "24729\n",
      "[    3     5     9 ... 25290 25291 25292]\n",
      "22256\n",
      "train... logistic regression\n",
      "RECALL:  0.3581902517487749\n",
      "PRECISION:  0.47881205673758864\n",
      "F_MEASURE:  0.333448306835534\n",
      "ACCURACY:  0.8317152103559871\n",
      "CONFUSION MATRIX:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "highset = np.array(df_high_agreement['class'].keys())\n",
    "print(len(highset))\n",
    "\n",
    "allset = np.array(list(df_dict['class'].keys())) \n",
    "print(len(allset))\n",
    "#IDs of tweets on which there were no sub. agreement:\n",
    "noagreement = np.setdiff1d(allset,highset)\n",
    "print(noagreement)\n",
    "devvy = np.array(dev_set)\n",
    "print(len(devvy))\n",
    "\n",
    "\n",
    "newtest = []\n",
    "for x in test_set:\n",
    "    if allset[x] not in highset:\n",
    "        newtest.append(x)\n",
    "        \n",
    "newcm, newcm2 = test_model(log_reg, bow_features_set, dev_set, newtest)\n",
    "\n",
    "#hatespeech based confusion matrix:\n",
    "plt.imshow(cm)\n",
    "#overall confusion matrix:\n",
    "plt.imshow(cm2)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "T. Davidson, D. Warmsley, M.W. Macy & I. Weber (2017). Automated Hate Speech Detection and the Problem of Offensive Language. In *Proceedings of the Eleventh International Conference on Web and Social Media, ICWSM 2017*: 512-515.\n",
    "\n",
    "J.T. Nockleby (2000). Hate Speech. In L.W. Levy, K.L. Karst et al. (eds.), *Encyclopedia of the American Constitution*, New York: Macmillan, pp. 1277-1279.\n",
    "\n",
    "A. Schmidt & M. Wiegand (2017). A Survey on Hate Speech Detection using Natural Language Processing. In *Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media*: 1-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
